{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5ca9b68",
   "metadata": {},
   "source": [
    "# Batch 2 DEXA Data Cleaning\n",
    "\n",
    "**Objective**: Extract and organize DEXA scan data from Batch 2 text files\n",
    "\n",
    "**Data Source**: /Sample Data/DEXA Scans/Batch 2\n",
    "\n",
    "**Output**: Organized Excel file with all measurements by subject, gender, and timepoint\n",
    "\n",
    "**Process**: Scan → Parse → Organize → Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead4eb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# Data paths\n",
    "batch2_path = Path(\"/Users/aviado/Documents/GDG WashU Medicine/Sample Data/DEXA Scans/Batch 2\")\n",
    "output_dir = Path(\"../../../cleaned_output\")\n",
    "output_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a94d4e",
   "metadata": {},
   "source": [
    "## Data Scanning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5587e3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan all txt files in Batch 2\n",
    "def scan_batch2_files():\n",
    "    txt_files = []\n",
    "    \n",
    "    # Define timepoint directories for Batch 2\n",
    "    timepoints = {\n",
    "        'Pre-Scan': 'Pre_Scan',\n",
    "        'WEEK1': 'Week_1',\n",
    "        'Week2': 'Week_2', \n",
    "        'Week 3': 'Week_3',\n",
    "        'Post-Scan': 'Post_Scan'\n",
    "    }\n",
    "    \n",
    "    # Scan each timepoint directory\n",
    "    for timepoint_dir, timepoint_name in timepoints.items():\n",
    "        timepoint_path = batch2_path / timepoint_dir\n",
    "        if timepoint_path.exists():\n",
    "            # Check Male/MALE and Female/FEMALE subdirectories\n",
    "            for gender in ['Male', 'Female', 'MALE', 'FEMALE']:\n",
    "                gender_path = timepoint_path / gender\n",
    "                if gender_path.exists():\n",
    "                    # Get all txt files\n",
    "                    for txt_file in gender_path.glob('*.txt'):\n",
    "                        txt_files.append({\n",
    "                            'file_path': txt_file,\n",
    "                            'timepoint': timepoint_name,\n",
    "                            'gender': gender.capitalize(),\n",
    "                            'filename': txt_file.name\n",
    "                        })\n",
    "    \n",
    "    # Also check root directory for any txt files\n",
    "    for txt_file in batch2_path.glob('*.txt'):\n",
    "        txt_files.append({\n",
    "            'file_path': txt_file,\n",
    "            'timepoint': 'Root',\n",
    "            'gender': 'Unknown',\n",
    "            'filename': txt_file.name\n",
    "        })\n",
    "    \n",
    "    return txt_files\n",
    "\n",
    "# Scan files\n",
    "batch2_files = scan_batch2_files()\n",
    "print(f\"Found {len(batch2_files)} txt files in Batch 2\")\n",
    "\n",
    "# Show sample of found files\n",
    "for i, file_info in enumerate(batch2_files[:5]):\n",
    "    print(f\"{i+1}. {file_info['timepoint']} - {file_info['gender']} - {file_info['filename']}\")\n",
    "if len(batch2_files) > 5:\n",
    "    print(f\"... and {len(batch2_files) - 5} more files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05222f2f",
   "metadata": {},
   "source": [
    "## Data Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd6bc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse DEXA txt file content\n",
    "def parse_dexa_txt(file_path):\n",
    "    \"\"\"Extract measurements from DEXA txt file\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Extract subject ID from filename (adapt for Batch 2 naming)\n",
    "        filename = file_path.name\n",
    "        subject_match = re.search(r'(B2_[MF]_\\d+)', filename)\n",
    "        if not subject_match:\n",
    "            # Try alternative patterns for Batch 2\n",
    "            subject_match = re.search(r'([MF]\\d+)', filename)\n",
    "        subject_id = subject_match.group(1) if subject_match else filename.replace('.txt', '')\n",
    "        \n",
    "        # Extract measurements using regex patterns\n",
    "        measurements = {'subject_id': subject_id}\n",
    "        \n",
    "        # Find WHOLE TISSUE STATISTICS section (more comprehensive data)\n",
    "        whole_section = re.search(r'WHOLE TISSUE STATISTICS:(.*?)(?=\\n\\s*-|$)', content, re.DOTALL)\n",
    "        if whole_section:\n",
    "            section_text = whole_section.group(1)\n",
    "        else:\n",
    "            # Fallback to INSIDE ROI if WHOLE not found\n",
    "            section_text = content\n",
    "        \n",
    "        # Define patterns for key measurements\n",
    "        patterns = {\n",
    "            'sample_area': r'Sample Area:\\s*([\\d.]+)\\s*cm',\n",
    "            'bone_area': r'Bone Area:\\s*([\\d.]+)\\s*cm',\n",
    "            'total_weight': r'Total Weight:\\s*([\\d.]+)\\s*g',\n",
    "            'soft_weight': r'Soft Weight:\\s*([\\d.]+)\\s*g',\n",
    "            'lean_weight': r'Lean Weight:\\s*([\\d.]+)\\s*g',\n",
    "            'fat_weight': r'Fat Weight:\\s*([\\d.]+)\\s*g',\n",
    "            'fat_percent': r'Fat Percent:\\s*([\\d.]+)',\n",
    "            'bmc': r'BMC:\\s*([\\d.]+)\\s*g',\n",
    "            'bmd': r'BMD:\\s*([\\d.]+)\\s*mg/cm'\n",
    "        }\n",
    "        \n",
    "        # Extract each measurement\n",
    "        for key, pattern in patterns.items():\n",
    "            match = re.search(pattern, section_text)\n",
    "            if match:\n",
    "                measurements[key] = float(match.group(1))\n",
    "            else:\n",
    "                measurements[key] = None\n",
    "        \n",
    "        return measurements\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test parsing with first file\n",
    "if batch2_files:\n",
    "    sample_file = batch2_files[0]['file_path']\n",
    "    sample_data = parse_dexa_txt(sample_file)\n",
    "    print(f\"Sample parsing result:\")\n",
    "    print(f\"File: {sample_file.name}\")\n",
    "    for key, value in sample_data.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489aa4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all Batch 2 files\n",
    "def process_all_batch2_files():\n",
    "    all_data = []\n",
    "    \n",
    "    for file_info in batch2_files:\n",
    "        # Parse the txt file\n",
    "        measurements = parse_dexa_txt(file_info['file_path'])\n",
    "        \n",
    "        if measurements:\n",
    "            # Add metadata\n",
    "            measurements.update({\n",
    "                'batch': 'Batch_2',\n",
    "                'timepoint': file_info['timepoint'], \n",
    "                'gender': file_info['gender'],\n",
    "                'filename': file_info['filename']\n",
    "            })\n",
    "            all_data.append(measurements)\n",
    "    \n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "# Process all files\n",
    "batch2_df = process_all_batch2_files()\n",
    "\n",
    "print(f\"Processed {len(batch2_df)} files\")\n",
    "print(f\"Columns: {list(batch2_df.columns)}\")\n",
    "print(f\"Shape: {batch2_df.shape}\")\n",
    "\n",
    "# Show sample data\n",
    "batch2_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec691a9",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ff5d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the organized data\n",
    "print(\"Batch 2 Data Summary:\")\n",
    "print(f\"Total subjects: {batch2_df['subject_id'].nunique()}\")\n",
    "print(f\"Timepoints: {batch2_df['timepoint'].unique()}\")\n",
    "print(f\"Gender distribution: {batch2_df['gender'].value_counts().to_dict()}\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_data = batch2_df.isnull().sum()\n",
    "numeric_cols = batch2_df.select_dtypes(include=[np.number]).columns\n",
    "print(f\"\\nMissing values in key measurements:\")\n",
    "for col in numeric_cols:\n",
    "    if missing_data[col] > 0:\n",
    "        print(f\"  {col}: {missing_data[col]} missing\")\n",
    "\n",
    "# Subject tracking across timepoints\n",
    "subject_timepoints = batch2_df.groupby('subject_id')['timepoint'].nunique().sort_values(ascending=False)\n",
    "print(f\"\\nSubject longitudinal tracking:\")\n",
    "print(f\"Subjects with multiple timepoints: {len(subject_timepoints[subject_timepoints > 1])}\")\n",
    "print(f\"Most tracked subject has {subject_timepoints.iloc[0]} timepoints\")\n",
    "\n",
    "# Show distribution by timepoint and gender\n",
    "timepoint_gender = batch2_df.groupby(['timepoint', 'gender']).size().unstack(fill_value=0)\n",
    "print(f\"\\nScans by timepoint and gender:\")\n",
    "print(timepoint_gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1a4333",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5530fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the Batch 2 dataset\n",
    "def clean_batch2_data(df):\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Fill missing numeric values with median (more appropriate for DEXA measurements)\n",
    "    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if df_clean[col].isnull().sum() > 0:\n",
    "            if df_clean[col].notna().sum() > 0:\n",
    "                median_val = df_clean[col].median()\n",
    "                df_clean[col] = df_clean[col].fillna(median_val)\n",
    "            else:\n",
    "                df_clean[col] = df_clean[col].fillna(0)\n",
    "    \n",
    "    # Fill missing categorical values\n",
    "    categorical_cols = df_clean.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        if df_clean[col].isnull().sum() > 0:\n",
    "            df_clean[col] = df_clean[col].fillna(\"Unknown\")\n",
    "    \n",
    "    # Organize columns in logical order\n",
    "    column_order = [\n",
    "        'batch', 'subject_id', 'timepoint', 'gender', 'filename',\n",
    "        'total_weight', 'soft_weight', 'lean_weight', 'fat_weight', 'fat_percent',\n",
    "        'bmc', 'bmd', 'bone_area', 'sample_area'\n",
    "    ]\n",
    "    \n",
    "    # Reorder columns (keep any extra columns at the end)\n",
    "    available_cols = [col for col in column_order if col in df_clean.columns]\n",
    "    extra_cols = [col for col in df_clean.columns if col not in column_order]\n",
    "    df_clean = df_clean[available_cols + extra_cols]\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Clean the data\n",
    "batch2_cleaned = clean_batch2_data(batch2_df)\n",
    "\n",
    "print(f\"Data cleaning complete\")\n",
    "print(f\"Missing values remaining: {batch2_cleaned.isnull().sum().sum()}\")\n",
    "print(f\"Final shape: {batch2_cleaned.shape}\")\n",
    "\n",
    "# Show cleaned data sample\n",
    "batch2_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04e309f",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9c135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to Excel with multiple sheets\n",
    "excel_output_path = output_dir / \"batch2_dexa_cleaned.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(excel_output_path, engine='openpyxl') as writer:\n",
    "    # Main data sheet\n",
    "    batch2_cleaned.to_excel(writer, sheet_name='Batch2_All_Data', index=False)\n",
    "    \n",
    "    # Summary by timepoint\n",
    "    timepoint_summary = batch2_cleaned.groupby(['timepoint', 'gender']).agg({\n",
    "        'subject_id': 'nunique',\n",
    "        'total_weight': 'mean',\n",
    "        'fat_percent': 'mean',\n",
    "        'bmd': 'mean',\n",
    "        'lean_weight': 'mean'\n",
    "    }).round(3)\n",
    "    timepoint_summary.to_excel(writer, sheet_name='Timepoint_Summary')\n",
    "    \n",
    "    # Subject tracking sheet\n",
    "    subject_summary = batch2_cleaned.groupby('subject_id').agg({\n",
    "        'timepoint': 'nunique',\n",
    "        'gender': 'first',\n",
    "        'total_weight': ['min', 'max', 'mean'],\n",
    "        'fat_percent': ['min', 'max', 'mean']\n",
    "    }).round(3)\n",
    "    subject_summary.columns = ['_'.join(col).strip() for col in subject_summary.columns]\n",
    "    subject_summary.to_excel(writer, sheet_name='Subject_Summary')\n",
    "\n",
    "print(f\"Excel file exported: {excel_output_path}\")\n",
    "print(f\"Sheets created: Batch2_All_Data, Timepoint_Summary, Subject_Summary\")\n",
    "\n",
    "# Also save as CSV for backup\n",
    "csv_output_path = output_dir / \"batch2_dexa_cleaned.csv\"\n",
    "batch2_cleaned.to_csv(csv_output_path, index=False)\n",
    "print(f\"CSV backup saved: {csv_output_path}\")\n",
    "\n",
    "print(f\"\\nFinal Results Summary:\")\n",
    "print(f\"- Total records: {len(batch2_cleaned)}\")\n",
    "print(f\"- Unique subjects: {batch2_cleaned['subject_id'].nunique()}\")\n",
    "print(f\"- Timepoints: {list(batch2_cleaned['timepoint'].unique())}\")\n",
    "print(f\"- Gender distribution: {batch2_cleaned['gender'].value_counts().to_dict()}\")\n",
    "print(f\"- Key measurements: total_weight, fat_percent, bmd, lean_weight, fat_weight\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
